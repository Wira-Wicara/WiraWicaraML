{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model Process for Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pydub\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, Conv1D, Flatten, Dense, Lambda, Dropout, MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess(file_path, target_length=8000):\n",
    "    audio, _ = librosa.load(file_path, sr=8000, mono=True)\n",
    "    \n",
    "    # Ensure audio length is not greater than target_length\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        # Pad audio to target_length if shorter\n",
    "        pad_amount = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, pad_amount), mode='constant')\n",
    "    \n",
    "    # Normalize audio\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # Reshape audio to include time steps dimension\n",
    "    audio = np.expand_dims(audio, axis=-1)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load augmented dataset\n",
    "def load_data(base_dir):\n",
    "    sentences = []\n",
    "    file_paths = []\n",
    "\n",
    "    for sentence in os.listdir(base_dir):\n",
    "        sentence_dir = os.path.join(base_dir, sentence)\n",
    "        for file in os.listdir(sentence_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_paths.append(os.path.join(sentence_dir, file))\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    return np.array(file_paths), np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert and normalize audio files\n",
    "def convert_and_normalize(input_path, output_path, target_dBFS=0, target_sample_rate=8000):\n",
    "    audio = pydub.AudioSegment.from_file(input_path)\n",
    "    change_in_dBFS = target_dBFS - audio.dBFS\n",
    "    normalized_audio = audio.apply_gain(change_in_dBFS)\n",
    "    normalized_audio = normalized_audio.set_frame_rate(target_sample_rate)\n",
    "    normalized_audio.export(output_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create pairs of audio samples with their labels\n",
    "def create_pairs(files, sentences):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    num_samples = len(files)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):\n",
    "            if sentences[i] == sentences[j]:\n",
    "                pairs.append((i, j))\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                pairs.append((i, j)) \n",
    "                labels.append(0)\n",
    "    \n",
    "    return np.array(pairs, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(files, sentences, batch_size=32, target_length=8000):\n",
    "    while True:\n",
    "        indices = np.random.permutation(len(files))\n",
    "        pairs, labels = create_pairs(files, sentences)\n",
    "        batch_start = 0\n",
    "        while batch_start < len(pairs):\n",
    "            batch_end = min(batch_start + batch_size, len(pairs))\n",
    "            batch_indices = indices[batch_start:batch_end]\n",
    "            batch_pairs = pairs[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            \n",
    "            audio_1 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 0]])\n",
    "            audio_2 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 1]])\n",
    "            \n",
    "            if len(batch_pairs) == 0:\n",
    "                break\n",
    "            \n",
    "            yield [audio_1, audio_2], batch_labels\n",
    "            batch_start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Siamese CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Siamese CNN model\n",
    "def create_siamese_model(input_shape):\n",
    "    def cnn_network(input_shape):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Conv1D(64, 5, activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Conv1D(128, 5, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "        model.add(Dropout(0.5))\n",
    "        return model\n",
    "\n",
    "    input_left = Input(shape=input_shape)\n",
    "    input_right = Input(shape=input_shape)\n",
    "\n",
    "    cnn = cnn_network(input_shape)\n",
    "\n",
    "    encoded_left = cnn(input_left)\n",
    "    encoded_right = cnn(input_right)\n",
    "\n",
    "    L1_distance = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
    "    L1_distance_out = L1_distance([encoded_left, encoded_right])\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance_out)\n",
    "\n",
    "    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)\n",
    "\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented dataset\n",
    "output_dir = \"fused\"\n",
    "\n",
    "file_paths, sentences = load_data(output_dir)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_files, val_files, train_sentences, val_sentences = train_test_split(file_paths, sentences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generators and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_gen = data_generator(train_files, train_sentences, batch_size=32, target_length=8000)\n",
    "val_gen = data_generator(val_files, val_sentences, batch_size=32, target_length=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (8000, 1)\n",
    "siamese_model = create_siamese_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for training\n",
    "checkpoint = ModelCheckpoint('siamese_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "steps_per_epoch = len(train_files) // 32\n",
    "validation_steps = len(val_files) // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese_model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muat model Keras dari file .h5\n",
    "siamese_model = tf.keras.models.load_model('siamese_model.h5')\n",
    "\n",
    "# Buat konverter TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
    "\n",
    "# Mengatur optimasi ke kuantisasi float16\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "# Konversi model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan model TFLite ke file\n",
    "with open('model_quant_float16.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model berhasil dikonversi dan disimpan sebagai model_quant_float16.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the test audio files (m4a format)\n",
    "test_audio_file_1 = 'test/path-to-audio.m4a'\n",
    "test_audio_file_2 = 'test/path-to-audio.m4a'\n",
    "\n",
    "# Paths to the converted WAV files\n",
    "converted_audio_file_1 = 'converted_test_audio_1.wav'\n",
    "converted_audio_file_2 = 'converted_test_audio_2.wav'\n",
    "\n",
    "# Convert the test audio files to WAV format with 8kHz sample rate\n",
    "convert_and_normalize(test_audio_file_1, converted_audio_file_1)\n",
    "convert_and_normalize(test_audio_file_2, converted_audio_file_2)\n",
    "\n",
    "# Load and preprocess the test audio files\n",
    "test_audio_1 = load_and_preprocess(converted_audio_file_1, target_length=8000)\n",
    "test_audio_2 = load_and_preprocess(converted_audio_file_2, target_length=8000)\n",
    "\n",
    "# Add batch dimension\n",
    "test_audio_1 = np.expand_dims(test_audio_1, axis=0)\n",
    "test_audio_2 = np.expand_dims(test_audio_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Siamese model\n",
    "siamese_model = tf.keras.models.load_model('siamese_model.h5', compile=False)\n",
    "\n",
    "# Predict the similarity\n",
    "similarity_score = siamese_model([test_audio_1, test_audio_2])\n",
    "\n",
    "# Define the min and max scores for normalization\n",
    "min_score = 0.4\n",
    "max_score = 0.5\n",
    "\n",
    "# Normalize the similarity score to the range 0-100%\n",
    "normalized_similarity_score = (similarity_score[0][0] - min_score) / (max_score - min_score) * 100\n",
    "\n",
    "# Clip the value to ensure it stays within the 0-100% range\n",
    "normalized_similarity_score = np.clip(normalized_similarity_score, 0, 100)\n",
    "\n",
    "# Output the normalized similarity score\n",
    "print(f'Similarity score: {normalized_similarity_score:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
