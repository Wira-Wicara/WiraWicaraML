{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model Process for Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Flatten, Dense, Lambda, Dropout, MaxPooling1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess audio files\n",
    "def load_and_preprocess(file_path, target_length=16000):\n",
    "    audio, _ = librosa.load(file_path, sr=16000, mono=True)\n",
    "    \n",
    "    # Ensure audio length is not greater than target_length\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        # Pad audio to target_length if shorter\n",
    "        pad_amount = target_length - len(audio)\n",
    "        audio = np.pad(audio, (0, pad_amount), mode='constant')\n",
    "    \n",
    "    # Normalize audio\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # Reshape audio to include time steps dimension\n",
    "    audio = np.expand_dims(audio, axis=-1)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load augmented dataset\n",
    "def load_data(base_dir):\n",
    "    sentences = []\n",
    "    file_paths = []\n",
    "\n",
    "    for sentence in os.listdir(base_dir):\n",
    "        sentence_dir = os.path.join(base_dir, sentence)\n",
    "        for file in os.listdir(sentence_dir):\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_paths.append(os.path.join(sentence_dir, file))\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    return np.array(file_paths), np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create pairs of audio samples with their labels\n",
    "def create_pairs(files, sentences):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    num_samples = len(files)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for j in range(i+1, num_samples):\n",
    "            if sentences[i] == sentences[j]:\n",
    "                pairs.append((i, j))\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                pairs.append((i, j)) \n",
    "                labels.append(0)\n",
    "    \n",
    "    return np.array(pairs, dtype=np.int32), np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(files, sentences, batch_size=32, target_length=16000):\n",
    "    while True:\n",
    "        indices = np.random.permutation(len(files))\n",
    "        pairs, labels = create_pairs(files, sentences)\n",
    "        batch_start = 0\n",
    "        while batch_start < len(pairs):\n",
    "            batch_end = min(batch_start + batch_size, len(pairs))\n",
    "            batch_indices = indices[batch_start:batch_end]\n",
    "            batch_pairs = pairs[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            \n",
    "            audio_1 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 0]])\n",
    "            audio_2 = np.array([load_and_preprocess(files[i], target_length) for i in batch_pairs[:, 1]])\n",
    "            \n",
    "            if len(batch_pairs) == 0:\n",
    "                break\n",
    "            \n",
    "            yield [audio_1, audio_2], batch_labels\n",
    "            batch_start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Siamese CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the Siamese CNN model\n",
    "def create_siamese_model(input_shape):\n",
    "    def cnn_network(input_shape):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Conv1D(64, 5, activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Conv1D(128, 5, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        return model\n",
    "\n",
    "    input_left = Input(shape=input_shape)\n",
    "    input_right = Input(shape=input_shape)\n",
    "\n",
    "    cnn = cnn_network(input_shape)\n",
    "\n",
    "    encoded_left = cnn(input_left)\n",
    "    encoded_right = cnn(input_right)\n",
    "\n",
    "    L1_distance = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
    "    L1_distance_out = L1_distance([encoded_left, encoded_right])\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid')(L1_distance_out)\n",
    "\n",
    "    siamese_model = Model(inputs=[input_left, input_right], outputs=prediction)\n",
    "\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented dataset\n",
    "output_dir = \"tts_output_words\"\n",
    "\n",
    "file_paths, sentences = load_data(output_dir)\n",
    "\n",
    "# Sel Split data into training and validation sets\n",
    "train_files, val_files, train_sentences, val_sentences = train_test_split(file_paths, sentences, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generators and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators\n",
    "train_gen = data_generator(train_files, train_sentences, batch_size=32, target_length=16000)\n",
    "val_gen = data_generator(val_files, val_sentences, batch_size=32, target_length=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 16000, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 16000, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 256)                  1310154   ['input_5[0][0]',             \n",
      "                                                          24         'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)           (None, 256)                  0         ['sequential_2[0][0]',        \n",
      "                                                                     'sequential_2[1][0]']        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    257       ['lambda_2[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 131015681 (499.79 MB)\n",
      "Trainable params: 131015681 (499.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define input shape\n",
    "input_shape = (16000, 1)\n",
    "siamese_model = create_siamese_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for training\n",
    "checkpoint = ModelCheckpoint('siamese_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "steps_per_epoch = len(train_files) // 32\n",
    "validation_steps = len(val_files) // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese_model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Giat\\AppData\\Local\\Temp\\ipykernel_8568\\2072721133.py:5: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(siamese_model, 'siamese_model.h5')\n"
     ]
    }
   ],
   "source": [
    "# Load an existing model\n",
    "# model = load_model('siamese_model.h5')\n",
    "\n",
    "# Save the model to a new .h5 file\n",
    "# save_model(siamese_model, 'siamese_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Siamese model\n",
    "siamese_model = tf.keras.models.load_model('siamese_model.h5', compile=False)\n",
    "\n",
    "# Get the input shape of the model\n",
    "input_shapes = [input_layer.shape.as_list() for input_layer in siamese_model.inputs]\n",
    "\n",
    "# Replace None values with default dimensions (e.g., 224 for image size)\n",
    "for shape in input_shapes:\n",
    "    for i, dim in enumerate(shape):\n",
    "        if dim is None:\n",
    "            shape[i] = 224  # Replace with a default size, e.g., 224 for image dimensions\n",
    "\n",
    "# Verify the input shapes\n",
    "print(f\"Input shapes: {input_shapes}\")\n",
    "\n",
    "# Define a function to generate representative dataset\n",
    "def representative_dataset_gen():\n",
    "    num_calibration_steps = 100\n",
    "    for _ in range(num_calibration_steps):\n",
    "        # Generate a batch of input data\n",
    "        test_input_1 = np.random.rand(*input_shapes[0]).astype(np.float32)\n",
    "        test_input_2 = np.random.rand(*input_shapes[1]).astype(np.float32)\n",
    "        yield [test_input_1, test_input_2]\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "# Set the representative dataset for full integer quantization\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open('siamese_model_lite.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\MUHAMM~1\\AppData\\Local\\Temp\\tmpym5ey2sl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\MUHAMM~1\\AppData\\Local\\Temp\\tmpym5ey2sl\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil dikonversi dan disimpan sebagai model_quant_float16.tflite\n"
     ]
    }
   ],
   "source": [
    "# Muat model Keras dari file .h5\n",
    "siamese_model = tf.keras.models.load_model('siamese_model.h5')\n",
    "\n",
    "# Buat konverter TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
    "\n",
    "# Mengatur optimasi ke kuantisasi float16\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "# Konversi model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan model TFLite ke file\n",
    "with open('model_quant_float16.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model berhasil dikonversi dan disimpan sebagai model_quant_float16.tflite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the test audio files (wav format)\n",
    "test_audio_file_1 = 'converted_kemarin_jelas.wav'\n",
    "test_audio_file_2 = 'converted_kemarin_mayan.wav'\n",
    "\n",
    "# Load and preprocess the test audio files\n",
    "test_audio_1 = load_and_preprocess(test_audio_file_1, target_length=16000)\n",
    "test_audio_2 = load_and_preprocess(test_audio_file_2, target_length=16000)\n",
    "\n",
    "# Add batch dimension\n",
    "test_audio_1 = np.expand_dims(test_audio_1, axis=0)\n",
    "test_audio_2 = np.expand_dims(test_audio_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Siamese model\n",
    "siamese_model = tf.keras.models.load_model('siamese_model.h5', compile=False)\n",
    "\n",
    "# Predict the similarity\n",
    "similarity_score = siamese_model.predict([test_audio_1, test_audio_2])\n",
    "\n",
    "# Define the min and max scores for normalization\n",
    "min_score = 0.0\n",
    "max_score = 0.5\n",
    "\n",
    "# Normalize the similarity score to the range 0-100%\n",
    "normalized_similarity_score = (similarity_score[0][0] - min_score) / (max_score - min_score) * 100\n",
    "\n",
    "# Clip the value to ensure it stays within the 0-100% range\n",
    "normalized_similarity_score = np.clip(normalized_similarity_score, 0, 100)\n",
    "\n",
    "# Output the normalized similarity score\n",
    "print(f'Similarity score: {normalized_similarity_score:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
